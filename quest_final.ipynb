{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=6, micro=4, releaselevel='final', serial=0)\n",
      "Tensor(\"pool1/MaxPool:0\", shape=(?, 32, 32, 64), dtype=float32)\n",
      "Tensor(\"pool2/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "Tensor(\"pool3/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "Tensor(\"pool4/MaxPool:0\", shape=(?, 4, 4, 256), dtype=float32)\n",
      "Tensor(\"dense5/Relu:0\", shape=(?, 625), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-1-f7a7ae4b3bc0>:54: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "----------parameter count start-----------\n",
      "(3, 3, 1, 64)\n",
      "4\n",
      "3\n",
      "3\n",
      "1\n",
      "64\n",
      "576\n",
      "(64,)\n",
      "1\n",
      "64\n",
      "64\n",
      "(3, 3, 64, 64)\n",
      "4\n",
      "3\n",
      "3\n",
      "64\n",
      "64\n",
      "36864\n",
      "(64,)\n",
      "1\n",
      "64\n",
      "64\n",
      "(3, 3, 64, 128)\n",
      "4\n",
      "3\n",
      "3\n",
      "64\n",
      "128\n",
      "73728\n",
      "(128,)\n",
      "1\n",
      "128\n",
      "128\n",
      "(3, 3, 128, 256)\n",
      "4\n",
      "3\n",
      "3\n",
      "128\n",
      "256\n",
      "294912\n",
      "(256,)\n",
      "1\n",
      "256\n",
      "256\n",
      "(4096, 625)\n",
      "2\n",
      "4096\n",
      "625\n",
      "2560000\n",
      "(625,)\n",
      "1\n",
      "625\n",
      "625\n",
      "(625, 10)\n",
      "2\n",
      "625\n",
      "10\n",
      "6250\n",
      "(10,)\n",
      "1\n",
      "10\n",
      "10\n",
      "2973477\n",
      "----------parameter count end-----------\n",
      "Learning Started!\n",
      "Learning Rate:0.001, Epochs: 60, Batch size:100\n",
      "Epoch: 0001 cost = 3.201486023 2018-12-18 15:55:02.454124 Validation Set Accuracy:0.3480\n",
      "Epoch: 0002 cost = 1.703389588 2018-12-18 15:55:19.574712 Validation Set Accuracy:0.4260\n",
      "Epoch: 0003 cost = 1.575366677 2018-12-18 15:55:36.627084 Validation Set Accuracy:0.4760\n",
      "Epoch: 0004 cost = 1.455311652 2018-12-18 15:55:53.663269 Validation Set Accuracy:0.4980\n",
      "Epoch: 0005 cost = 1.383641276 2018-12-18 15:56:10.708124 Validation Set Accuracy:0.5340\n",
      "Epoch: 0006 cost = 1.312018322 2018-12-18 15:56:27.753807 Validation Set Accuracy:0.5640\n",
      "Epoch: 0007 cost = 1.249122731 2018-12-18 15:56:44.764744 Validation Set Accuracy:0.5980\n",
      "Epoch: 0008 cost = 1.208397619 2018-12-18 15:57:01.779438 Validation Set Accuracy:0.6060\n",
      "Epoch: 0009 cost = 1.143540503 2018-12-18 15:57:18.801525 Validation Set Accuracy:0.6020\n",
      "Epoch: 0010 cost = 1.129720739 2018-12-18 15:57:35.848073 Validation Set Accuracy:0.6140\n",
      "Epoch: 0011 cost = 1.082448781 2018-12-18 15:57:52.888012 Validation Set Accuracy:0.6140\n",
      "Epoch: 0012 cost = 1.062572111 2018-12-18 15:58:09.888086 Validation Set Accuracy:0.5960\n",
      "Epoch: 0013 cost = 1.031091102 2018-12-18 15:58:26.869835 Validation Set Accuracy:0.6520\n",
      "Epoch: 0014 cost = 1.011504843 2018-12-18 15:58:43.861936 Validation Set Accuracy:0.6360\n",
      "Epoch: 0015 cost = 0.979480305 2018-12-18 15:59:00.863249 Validation Set Accuracy:0.6640\n",
      "Epoch: 0016 cost = 0.959179145 2018-12-18 15:59:17.865804 Validation Set Accuracy:0.6440\n",
      "Epoch: 0017 cost = 0.935266343 2018-12-18 15:59:34.859024 Validation Set Accuracy:0.6000\n",
      "Epoch: 0018 cost = 0.937808888 2018-12-18 15:59:51.817283 Validation Set Accuracy:0.6480\n",
      "Epoch: 0019 cost = 0.898263727 2018-12-18 16:00:08.769344 Validation Set Accuracy:0.6640\n",
      "Epoch: 0020 cost = 0.876739304 2018-12-18 16:00:25.721293 Validation Set Accuracy:0.6560\n",
      "Epoch: 0021 cost = 0.863388630 2018-12-18 16:00:42.689991 Validation Set Accuracy:0.6420\n",
      "Epoch: 0022 cost = 0.837760003 2018-12-18 16:00:59.599426 Validation Set Accuracy:0.6400\n",
      "Epoch: 0023 cost = 0.836622064 2018-12-18 16:01:16.523125 Validation Set Accuracy:0.6780\n",
      "Epoch: 0024 cost = 0.821765979 2018-12-18 16:01:33.433283 Validation Set Accuracy:0.6740\n",
      "Epoch: 0025 cost = 0.805000605 2018-12-18 16:01:50.362210 Validation Set Accuracy:0.6600\n",
      "Epoch: 0026 cost = 0.779041140 2018-12-18 16:02:07.288237 Validation Set Accuracy:0.6740\n",
      "Epoch: 0027 cost = 0.766600757 2018-12-18 16:02:24.239089 Validation Set Accuracy:0.6740\n",
      "Epoch: 0028 cost = 0.752760381 2018-12-18 16:02:41.167945 Validation Set Accuracy:0.6600\n",
      "Epoch: 0029 cost = 0.743675784 2018-12-18 16:02:58.082027 Validation Set Accuracy:0.6560\n",
      "Epoch: 0030 cost = 0.719742602 2018-12-18 16:03:14.993694 Validation Set Accuracy:0.6700\n",
      "Epoch: 0031 cost = 0.718809680 2018-12-18 16:03:31.906086 Validation Set Accuracy:0.6860\n",
      "Epoch: 0032 cost = 0.713174307 2018-12-18 16:03:48.782430 Validation Set Accuracy:0.7040\n",
      "Epoch: 0033 cost = 0.684904758 2018-12-18 16:04:05.631252 Validation Set Accuracy:0.6520\n",
      "Epoch: 0034 cost = 0.686176722 2018-12-18 16:04:22.496767 Validation Set Accuracy:0.6620\n",
      "Epoch: 0035 cost = 0.670521700 2018-12-18 16:04:39.415518 Validation Set Accuracy:0.6660\n",
      "Epoch: 0036 cost = 0.665094887 2018-12-18 16:04:56.348704 Validation Set Accuracy:0.6540\n",
      "Epoch: 0037 cost = 0.666049479 2018-12-18 16:05:13.281802 Validation Set Accuracy:0.6720\n",
      "Epoch: 0038 cost = 0.642304124 2018-12-18 16:05:30.217951 Validation Set Accuracy:0.6760\n",
      "Epoch: 0039 cost = 0.625576169 2018-12-18 16:05:47.116379 Validation Set Accuracy:0.6760\n",
      "Epoch: 0040 cost = 0.620979362 2018-12-18 16:06:04.034731 Validation Set Accuracy:0.6940\n",
      "Epoch: 0041 cost = 0.627335475 2018-12-18 16:06:20.974873 Validation Set Accuracy:0.6740\n",
      "Epoch: 0042 cost = 0.607841342 2018-12-18 16:06:37.884813 Validation Set Accuracy:0.6800\n",
      "Epoch: 0043 cost = 0.611664114 2018-12-18 16:06:54.761954 Validation Set Accuracy:0.6680\n",
      "Epoch: 0044 cost = 0.591119460 2018-12-18 16:07:11.649571 Validation Set Accuracy:0.6760\n",
      "Epoch: 0045 cost = 0.590086854 2018-12-18 16:07:28.526581 Validation Set Accuracy:0.6620\n",
      "Epoch: 0046 cost = 0.583296513 2018-12-18 16:07:45.378875 Validation Set Accuracy:0.6620\n",
      "Epoch: 0047 cost = 0.579432148 2018-12-18 16:08:02.215102 Validation Set Accuracy:0.6920\n",
      "Epoch: 0048 cost = 0.569425051 2018-12-18 16:08:19.034657 Validation Set Accuracy:0.6840\n",
      "Epoch: 0049 cost = 0.552632548 2018-12-18 16:08:35.881599 Validation Set Accuracy:0.6840\n",
      "Epoch: 0050 cost = 0.549246064 2018-12-18 16:08:52.753201 Validation Set Accuracy:0.6760\n",
      "Epoch: 0051 cost = 0.554691319 2018-12-18 16:09:09.617322 Validation Set Accuracy:0.6760\n",
      "Epoch: 0052 cost = 0.546284271 2018-12-18 16:09:26.496089 Validation Set Accuracy:0.6740\n",
      "Epoch: 0053 cost = 0.555418763 2018-12-18 16:09:43.356020 Validation Set Accuracy:0.6840\n",
      "Epoch: 0054 cost = 0.538918727 2018-12-18 16:10:00.207720 Validation Set Accuracy:0.7160\n",
      "Epoch: 0055 cost = 0.528698627 2018-12-18 16:10:17.063231 Validation Set Accuracy:0.6980\n",
      "Epoch: 0056 cost = 0.513246768 2018-12-18 16:10:33.921096 Validation Set Accuracy:0.7040\n",
      "Epoch: 0057 cost = 0.535055352 2018-12-18 16:10:50.766437 Validation Set Accuracy:0.7000\n",
      "Epoch: 0058 cost = 0.512345096 2018-12-18 16:11:07.639884 Validation Set Accuracy:0.6880\n",
      "Epoch: 0059 cost = 0.513672788 2018-12-18 16:11:24.493638 Validation Set Accuracy:0.6860\n",
      "Epoch: 0060 cost = 0.503670488 2018-12-18 16:11:41.351827 Validation Set Accuracy:0.7140\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import read_stl10_file\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./logs/{}\".format(datetime.now().strftime(\"%Y-%m-%d-%H-%M\")))\n",
    "\n",
    "step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.001\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, step,\n",
    "                                                20000, 0.9, staircase=True)\n",
    "layers = {}\n",
    "\n",
    "\n",
    "training = tf.placeholder(tf.bool)\n",
    "X = tf.placeholder(tf.float32, [None, 96, 96, 1], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, 10], name='Y')\n",
    "\n",
    "conv1 = tf.layers.conv2d(inputs=X, filters=64, kernel_size=[3, 3],\n",
    "                         padding='SAME', activation=tf.nn.relu, name='conv1')\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[3, 3], padding='SAME', strides=3, name='pool1')\n",
    "dropout1 = tf.layers.dropout(inputs=pool1, rate=0.3, training=training, name='')\n",
    "layers['conv1'] = conv1\n",
    "print(pool1)\n",
    "\n",
    "conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n",
    "                         padding='SAME', activation=tf.nn.relu, name='conv2')\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], padding='SAME', strides=2, name='pool2')\n",
    "dropout2 = tf.layers.dropout(inputs=pool2, rate=0.3, training=training)\n",
    "layers['conv2'] = conv2\n",
    "print(pool2)\n",
    "\n",
    "conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n",
    "                         padding='SAME', activation=tf.nn.relu, name='conv3')\n",
    "pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], padding='SAME', strides=2, name='pool3')\n",
    "dropout3 = tf.layers.dropout(inputs=pool3, rate=0.3, training=training)\n",
    "layers['conv3'] = conv3\n",
    "print(pool3)\n",
    "\n",
    "conv4 = tf.layers.conv2d(inputs=dropout3, filters=256, kernel_size=[3, 3],\n",
    "                         padding='SAME', activation=tf.nn.relu, name='conv4')\n",
    "pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], padding='SAME', strides=2, name='pool4')\n",
    "dropout4 = tf.layers.dropout(inputs=pool4, rate=0.3, training=training)\n",
    "layers['conv4'] = conv4\n",
    "print(pool4)\n",
    "\n",
    "flat = tf.reshape(dropout4, [-1, 4 * 4 * 256])\n",
    "dense5 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu, name='dense5')\n",
    "dropout5 = tf.layers.dropout(inputs=dense5, rate=0.5, training=training)\n",
    "print(dense5)\n",
    "logits = tf.layers.dense(inputs=dropout5, units=10, name='logits')\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y), name='cost')\n",
    "optimizer = tf.train.AdamOptimizer(\n",
    "    learning_rate=learning_rate).minimize(cost, global_step=step)\n",
    "\n",
    "correct_prediction = tf.equal(\n",
    "    tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='pred')\n",
    "\n",
    "# tensorboard\n",
    "cost_summ = tf.summary.scalar(\"cost\", cost)\n",
    "accuracy_summ = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "def predict(sess, x_test, training_=False):\n",
    "    return sess.run(logits,\n",
    "                         feed_dict={X: x_test, training: training_})\n",
    "\n",
    "def get_accuracy(sess, x_test, y_test, training_=False):\n",
    "    return sess.run(accuracy,\n",
    "                         feed_dict={X: x_test,\n",
    "                                    Y: y_test, training: training_})\n",
    "\n",
    "def train(sess, x_data, y_data, training_=True):\n",
    "    summary, c, _ = sess.run([merged_summary, cost, optimizer], feed_dict={\n",
    "        X: x_data, Y: y_data, training: training_})\n",
    "    writer.add_summary(summary)\n",
    "    return c, _\n",
    "\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "def flip_images(X_imgs):\n",
    "    X_flip = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape=(96, 96, 1))\n",
    "    tf_img1 = tf.image.flip_left_right(X)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for img in X_imgs:\n",
    "            flipped_imgs = sess.run([tf_img1], feed_dict={X: img})\n",
    "            X_flip.extend(flipped_imgs)\n",
    "    X_flip = np.array(X_flip, dtype=np.float32)\n",
    "    return X_flip\n",
    "\n",
    "\n",
    "def central_scale_images(X_imgs, scales):\n",
    "    # Various settings needed for Tensorflow operation\n",
    "    boxes = np.zeros((len(scales), 4), dtype=np.float32)\n",
    "    for index, scale in enumerate(scales):\n",
    "        x1 = y1 = 0.5 - 0.5 * scale  # To scale centrally\n",
    "        x2 = y2 = 0.5 + 0.5 * scale\n",
    "        boxes[index] = np.array([y1, x1, y2, x2], dtype=np.float32)\n",
    "    box_ind = np.zeros((len(scales)), dtype=np.int32)\n",
    "    crop_size = np.array([96, 96], dtype=np.int32)\n",
    "\n",
    "    X_scale_data = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape=(1, 96, 96, 1))\n",
    "    # Define Tensorflow operation for all scales but only one base image at a time\n",
    "    tf_img = tf.image.crop_and_resize(X, boxes, box_ind, crop_size)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for img_data in X_imgs:\n",
    "            batch_img = np.expand_dims(img_data, axis=0)\n",
    "            scaled_imgs = sess.run(tf_img, feed_dict={X: batch_img})\n",
    "            X_scale_data.extend(scaled_imgs)\n",
    "\n",
    "    X_scale_data = np.array(X_scale_data, dtype=np.float32)\n",
    "    return X_scale_data\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "# parameter 개수 측정\n",
    "print(\"----------parameter count start-----------\")\n",
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    print(shape)\n",
    "    print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)\n",
    "print(\"----------parameter count end-----------\")\n",
    "\n",
    "# 데이터 불러오기, test set과 train set을 바꿔서 불러온다.\n",
    "images_test = read_stl10_file.read_all_images(read_stl10_file.DATA_PATH)\n",
    "labels_test = read_stl10_file.read_labels(read_stl10_file.LABEL_PATH)\n",
    "labels_test = labels_test - 1  # stl10 data의 index가 1부터 시작하므로, 1을 빼서 0을 start index로 설정.\n",
    "\n",
    "images_train = read_stl10_file.read_all_images(read_stl10_file.TEST_DATA_PATH)\n",
    "labels_train = read_stl10_file.read_labels(read_stl10_file.TEST_LABEL_PATH)\n",
    "labels_train = labels_train - 1  # stl10 data의 index가 1부터 시작하므로, 1을 빼서 0을 start index로 설정.\n",
    "\n",
    "# data transform\n",
    "t1 = tf.one_hot(labels_train, depth=10)\n",
    "t2 = tf.one_hot(labels_test, depth=10)\n",
    "i1 = tf.image.rgb_to_grayscale(images_train)\n",
    "i2 = tf.image.rgb_to_grayscale(images_test)\n",
    "labels_train_onehot, labels_test_onehot, images_train, images_test = sess.run([t1, t2, i1, i2])\n",
    "\n",
    "# data augmentation 과정\n",
    "flipped = flip_images(images_train)\n",
    "centered = central_scale_images(images_train, [0.9])\n",
    "images_train = np.vstack((images_train, flipped, centered))\n",
    "labels_train_onehot = np.vstack((labels_train_onehot, labels_train_onehot, labels_train_onehot))\n",
    "images_train, labels_train_onehot = unison_shuffled_copies(images_train, labels_train_onehot)\n",
    "\n",
    "# test set을 test set과 validation set으로 분리\n",
    "images_test_count = images_test.shape[0]\n",
    "test_valid_point = int(images_test_count * 0.9)\n",
    "\n",
    "labels_valid_onehot = labels_test_onehot[test_valid_point:]\n",
    "images_valid = images_test[test_valid_point:]\n",
    "\n",
    "labels_test_onehot = labels_test_onehot[:test_valid_point]\n",
    "images_test = images_test[:test_valid_point]\n",
    "\n",
    "# train, test set의 개수, (8000 * 3, 5000 * 0.9)\n",
    "images_train_count = images_train.shape[0]\n",
    "images_test_count = images_test.shape[0]\n",
    "images_valid_count = images_valid.shape[0]\n",
    "\n",
    "training_epochs = 60\n",
    "batch_size = 100\n",
    "\n",
    "print('Learning Started!')\n",
    "print(\"Learning Rate:{}, Epochs: {}, Batch size:{}\".format(starter_learning_rate, training_epochs, batch_size))\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    avg_accuracy = 0\n",
    "    total_batch = int(images_train_count / batch_size)\n",
    "    valid_batch = int(images_valid_count / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = images_train[i * batch_size:(i + 1) * batch_size], labels_train_onehot[\n",
    "                                                                                i * batch_size:(i + 1) * batch_size]\n",
    "        c, _ = train(sess, batch_xs, batch_ys)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    for i in range(valid_batch):\n",
    "        batch_xs, batch_ys = images_valid[i * batch_size:(i + 1) * batch_size], labels_valid_onehot[\n",
    "                                                                                i * batch_size:(i + 1) * batch_size]\n",
    "        acc = get_accuracy(sess, batch_xs, batch_ys)\n",
    "        avg_accuracy += acc / valid_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost), str(datetime.now()),\n",
    "          'Validation Set Accuracy:{:.4f}'.format(avg_accuracy))\n",
    "\n",
    "print('Learning Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 Accuracy: 0.688\n",
      "step 2 Accuracy: 0.695\n",
      "step 3 Accuracy: 0.699\n",
      "step 4 Accuracy: 0.725\n",
      "Test Accuracy: 0.7018\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 1000\n",
    "total_test_batch = int(images_test_count / test_batch_size)\n",
    "\n",
    "acc_sum = 0\n",
    "for i in range(total_test_batch):\n",
    "    batch_xs, batch_ys = images_test[i*test_batch_size:(i+1)*test_batch_size], labels_test_onehot[i*test_batch_size:(i+1)*test_batch_size]\n",
    "    acc = get_accuracy(sess, batch_xs, batch_ys)\n",
    "    print('step', i+1, 'Accuracy:', acc)\n",
    "    acc_sum += acc\n",
    "\n",
    "print(\"Test Accuracy: {:.4f}\".format(acc_sum / total_test_batch) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/final_model'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, './model/final_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
